\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Lecture 2: RL and Markov decision processes}
\author{ZHANG Mofan}
\date{February 2021}

\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{xcolor}
\usepackage{url}
\setlength\parindent{0pt}

\begin{document}

\maketitle
\setcounter{section}{-1}
\section{Resources}
UCL course on RL: \url{https://www.davidsilver.uk/teaching/}

Chinese blogs: \url{https://www.cnblogs.com/pinard/category/1254674.html}

\section{Notation}
\begin{tabular}{lp{0.6\textwidth}}
$\mathcal{P}_{ss'}^{\textcolor{red}{a}} = \mathbb{P}[S_{t+1} = s'|S_t = s, {\color{red}A_t = a}]$ & state transition probability\\

$\mathcal{P}$ & state transition probability matrix\\

$\mathcal{S}$ & finite set of states\\

\color{red} $\mathcal{A}$ & finite set of actions\\

$\mathcal{R}$ & reward function, $\mathcal{R}_{s}^{\textcolor{red}{a}} = \mathbb{E}[R_{t+1}|S_t = s, {\color{red}A_t = a}]$\\

$\gamma$ & discount factor $\in [0, 1]$\\

$G_t = \sum^{\infty}_{k=0} \gamma^kR_{t+k+1}$ & total discounted reward from time step t\\

$v(s) = \mathbb{E}[G_t|S_t = s]$ & state value function\\
\end{tabular}\\

\section{Bellman Equation for Markov reward processes}
\subsection{Decomposition of state value function}
\begin{itemize}
    \item[-] immediate reward $R_{t+1}$
    \item[-] discounted value of successor state $\gamma v(S_{t+1})$
\end{itemize}

\begin{equation}
\begin{aligned}
    v(s) &= \mathbb{E}[G_t | S_t = s]\\
         &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]\\
         &= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...) | S_t = s]\\
         &= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s]\\
         &= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
\end{aligned}
\end{equation}

Written in another form:
\begin{equation}
    v(s) = \mathcal{R}_s + \gamma \sum_{s'\in\mathcal{S}} \mathcal{P}_{ss'}v(s')
\end{equation}

\subsection{Matrix formulation}
\begin{equation}
    v = \mathcal{R} + \gamma \mathcal{P} v
\end{equation}

Solution:
\begin{equation}
    v = (\mathcal{I} - \gamma \mathcal{P})^{-1} \mathcal{R}
\end{equation}

\section{Policies in Markov decision processes}
A policy $\pi$ is a distribution over actions given states,
\subsection{From MRP to MDP}
\begin{equation}
    \pi(a|s) = \mathbb{P}[A_t = a | S_t = s]
\end{equation}

Definition:

\begin{equation}
\mathcal{P}^{\pi}_{s, s'} = \sum_{a\in\mathcal{A}} \pi(a|s) \mathcal{P}^a_{ss'}
\end{equation}

\begin{equation}
\mathcal{R}^{\pi}_{s} = \sum_{a\in\mathcal{A}}\pi(a|s) \mathcal{R}^a_s
\end{equation}

The state value function under policy $\pi$:

\begin{equation}
\begin{aligned}
    v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t | S_t = s]\\
    &= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
\end{aligned}
\end{equation}

The action value function under policy $\pi$:

\begin{equation}
\begin{aligned}
    q_{\pi}(s, a) &= \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]\\
    &= \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
\end{aligned}
\end{equation}

The relation between $v_{\pi}(s)$ and $q_{\pi}(s, a)$ is:

\begin{equation}
\begin{aligned}
    v_{\pi}(s) &= \sum_{a\in\mathcal{A}} \pi(a|s) q_{\pi}(s, a)\\
    &= \sum_{a\in\mathcal{A}} \pi(a|s) (\mathcal{R}^a_s + \gamma \sum_{s'\in\mathcal{S}} \mathcal{P}^a_{ss'}v_{\pi}(s'))
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
    q_{\pi}(s, a) &= \mathcal{R}_s^a + \gamma \sum_{s'\in\mathcal{S}}\mathcal{P}^a_{ss'}v_{\pi}(s')\\
    &= \mathcal{R}_s^a + \gamma \sum_{s'\in\mathcal{S}}\mathcal{P}^a_{ss'}\sum_{a'\in\mathcal{A}} \pi(a'|s') q_{\pi}(s', a')
\end{aligned}
\end{equation}

\subsection{Matrix form}
\begin{equation}
    v_{\pi} = \mathcal{R}^{\pi} + \gamma \mathcal{P}^{\pi}v_{\pi}
\end{equation}

Direct solution:
\begin{equation}
    v_{\pi} = (\mathcal{I} - \gamma \mathcal{P}^{\pi})^{-1} \mathcal{R}^{\pi}
\end{equation}

\section{Optimal value}
\subsection{Optimal value functions}
Optimal state value function:
\begin{equation}
    v_{*}(s) = \max_{\pi}v_{\pi}(s)
\end{equation}

Optimal action value function:
\begin{equation}
    q_{*}(s, a) = \max_{\pi}q_{\pi}(s, a)
\end{equation}

\subsection{Optimal policy}
Define a partial ordering over policies:
\begin{equation}
    \pi \geq \pi' \qquad \text{if} \quad v_{\pi}(s) \geq v_{\pi'}(s) \quad \forall s
\end{equation}

Theorem: For any MDP,
\begin{itemize}
    \item[1.] There exists an optimal policy $\pi_*$ that is better than or equal to all other policies, $\pi_* \geq \pi, \forall \pi$;
    \item[2.] All optimal policies achieve the optimal state value function, $v_{\pi_*}(s) = v_*(s)$;
    \item[3.] All optimal policies achieve the optimal action value function, $q_{\pi_*}(s, a) = q_*(s, a)$.
\end{itemize}

In practice, an optimal policy can be found by maximising over $q_*(s, a)$,

\begin{equation}
    \pi_*(a|s) = \left\{
    \begin{array}{cl}
       1  &  \text{if } a = \argmax_{a\in\mathcal{A}} q_*(s, a)\\
       0  & \text{otherwise}
    \end{array}
    \right.
\end{equation}

\end{document}


